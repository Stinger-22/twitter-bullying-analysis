{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4578ac3-8e26-4e36-9d01-df99583b6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SKLearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Text cleaning\n",
    "import re\n",
    "import string\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from langdetect import detect, LangDetectException\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f88de98e-417d-4b4e-b38d-032f1fcd0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words for text cleaning\n",
    "stop_words = set(stopwords.words('english') + ['rt', 'mkr', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 't', 's', 'https', 'http', 'co'])\n",
    "# Initialize lemmatizer for text cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Set seed for reproducibility\n",
    "seed_value = 22\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f99f2f2-4c50-4e24-b6d2-331047531f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25304f67-55e0-4566-9052-8c00fb898123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6be3d22b-1af0-40cd-9432-3bca2ef7e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "# Remove punctuations, stopwords, links, mentions and new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n",
    "    banned_list = string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    # Remove hashtags at the end of the sentence\n",
    "    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n",
    "    \n",
    "    # Remove the # symbol from hashtags in the middle of the sentence\n",
    "    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n",
    "    \n",
    "    return new_tweet\n",
    "\n",
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(text):\n",
    "    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "# Function to check if the text is in English, and return an empty string if it's not\n",
    "def filter_non_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"unknown\"\n",
    "    return text if lang == \"en\" else \"\"\n",
    "\n",
    "# Expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Lemmatize words\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Remove short words\n",
    "def remove_short_words(text, min_len=2):\n",
    "    words = text.split()\n",
    "    long_words = [word for word in words if len(word) >= min_len]\n",
    "    return ' '.join(long_words)\n",
    "\n",
    "# Replace elongated words with their base form\n",
    "def replace_elongated_words(text):\n",
    "    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n",
    "    return re.sub(regex_pattern, r'\\1\\3\\4', text)\n",
    "\n",
    "# Remove repeated punctuation\n",
    "def remove_repeated_punctuation(text):\n",
    "    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "\n",
    "# Remove extra whitespace\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_url_shorteners(text):\n",
    "    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)\n",
    "\n",
    "# Remove spaces at the beginning and end of the tweet\n",
    "def remove_spaces_tweets(tweet):\n",
    "    return tweet.strip()\n",
    "\n",
    "# Remove short tweets\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\"\n",
    "\n",
    "# Function to call all the cleaning functions in the correct order\n",
    "def clean_tweet(tweet):\n",
    "    tweet = strip_emoji(tweet)\n",
    "    tweet = expand_contractions(tweet)\n",
    "    tweet = filter_non_english(tweet)\n",
    "    tweet = strip_all_entities(tweet)\n",
    "    tweet = clean_hashtags(tweet)\n",
    "    tweet = filter_chars(tweet)\n",
    "    tweet = remove_mult_spaces(tweet)\n",
    "    tweet = remove_numbers(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    tweet = remove_short_words(tweet)\n",
    "    tweet = replace_elongated_words(tweet)\n",
    "    tweet = remove_repeated_punctuation(tweet)\n",
    "    tweet = remove_extra_whitespace(tweet)\n",
    "    tweet = remove_url_shorteners(tweet)\n",
    "    tweet = remove_spaces_tweets(tweet)\n",
    "    tweet = remove_short_tweets(tweet)\n",
    "    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bfcd59c-4d76-4502-bde8-73ab5ec3db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = [clean_tweet(tweet) for tweet in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f66d935b-d117-4ce2-954b-f1449c9e30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"text_clean\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "72b99d90-379d-49a3-8614-38269f7b424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_len'] = [len(text.split()) for text in df.text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "497fb773-7bea-4817-94b3-a853f9a42bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text_len'] < df['text_len'].quantile(0.995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dcb6bb65-e5f1-44fd-bfdc-f8a6650f91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4, 'other_cyberbullying':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a0cd7ea-30d9-4f69-ac64-d245105527bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "825f9a4b-23e7-4978-8bba-cd43fb0ecf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 4772],\n",
       "       [   1, 4715],\n",
       "       [   2, 4441],\n",
       "       [   3, 4352],\n",
       "       [   4, 3273],\n",
       "       [   5, 3337]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3eb570f2-a714-4ea6-a2d4-3110ed15f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1));\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'sentiment']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7b399bb-d742-4470-8b6b-18dfee03669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_os['text_clean'].values\n",
    "y_train = train_os['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44e07334-96ca-4d38-857f-1f2b28936033",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CountVectorizer()\n",
    "X_train_cv =  clf.fit_transform(X_train)\n",
    "X_test_cv = clf.transform(X_test)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n",
    "X_train_tf = tf_transformer.transform(X_train_cv)\n",
    "X_test_tf = tf_transformer.transform(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4907a719-669f-4ee7-a84d-d4b46e96c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\", \"other_cyberbullying\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b18bec9-19e2-421a-8a70-cf800eb093d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for GradientBoostingClassifier:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           religion       0.97      0.92      0.94      1492\n",
      "                age       0.98      0.96      0.97      1474\n",
      "          ethnicity       0.99      0.98      0.99      1387\n",
      "             gender       0.93      0.85      0.88      1360\n",
      "       not bullying       0.49      0.71      0.58      1023\n",
      "other_cyberbullying       0.62      0.49      0.54      1043\n",
      "\n",
      "           accuracy                           0.84      7779\n",
      "          macro avg       0.83      0.82      0.82      7779\n",
      "       weighted avg       0.86      0.84      0.84      7779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(random_state = seed_value)\n",
    "gb_classifier.fit(X_train_tf, y_train)\n",
    "gb_classifier_y_pred = gb_classifier.predict(X_test_tf)\n",
    "print('Classification Report for GradientBoostingClassifier:\\n',classification_report(y_test, gb_classifier_y_pred, target_names=sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3e00919b-60cd-40e4-811f-92b488007da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gb_classifier, open('../model/finalized_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "318bb007-3854-4f93-975b-0f23f6d43ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('../model/clf.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "edcc2e63-8d93-409f-b9de-1410a3403ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_transformer, open('../model/tf_transformer.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1df215-67ab-4d07-aa80-f1c1be7f9714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
