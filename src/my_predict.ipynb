{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb12ac2-dcc9-481e-89fe-348a44be8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SKLearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Text cleaning\n",
    "import re\n",
    "import string\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from langdetect import detect, LangDetectException\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4483d3-4ddf-4eb5-8591-6d65f7727fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words for text cleaning\n",
    "stop_words = set(stopwords.words('english') + ['rt', 'mkr', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', 'u', 'ur', 'don', 't', 's', 'https', 'http', 'co'])\n",
    "# Initialize lemmatizer for text cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Set seed for reproducibility\n",
    "seed_value = 22\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b187de6-86e9-4936-a534-f039fcffcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e440eff-6d13-4e28-a30b-3a70349d07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "# Remove punctuations, stopwords, links, mentions and new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n",
    "    banned_list = string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    # Remove hashtags at the end of the sentence\n",
    "    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n",
    "    \n",
    "    # Remove the # symbol from hashtags in the middle of the sentence\n",
    "    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n",
    "    \n",
    "    return new_tweet\n",
    "\n",
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(text):\n",
    "    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "# Function to check if the text is in English, and return an empty string if it's not\n",
    "def filter_non_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"unknown\"\n",
    "    return text if lang == \"en\" else \"\"\n",
    "\n",
    "# Expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Lemmatize words\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Remove short words\n",
    "def remove_short_words(text, min_len=2):\n",
    "    words = text.split()\n",
    "    long_words = [word for word in words if len(word) >= min_len]\n",
    "    return ' '.join(long_words)\n",
    "\n",
    "# Replace elongated words with their base form\n",
    "def replace_elongated_words(text):\n",
    "    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n",
    "    return re.sub(regex_pattern, r'\\1\\3\\4', text)\n",
    "\n",
    "# Remove repeated punctuation\n",
    "def remove_repeated_punctuation(text):\n",
    "    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "\n",
    "# Remove extra whitespace\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_url_shorteners(text):\n",
    "    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)\n",
    "\n",
    "# Remove spaces at the beginning and end of the tweet\n",
    "def remove_spaces_tweets(tweet):\n",
    "    return tweet.strip()\n",
    "\n",
    "# Remove short tweets\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\"\n",
    "\n",
    "# Function to call all the cleaning functions in the correct order\n",
    "def clean_tweet(tweet):\n",
    "    tweet = strip_emoji(tweet)\n",
    "    tweet = expand_contractions(tweet)\n",
    "    tweet = filter_non_english(tweet)\n",
    "    tweet = strip_all_entities(tweet)\n",
    "    tweet = clean_hashtags(tweet)\n",
    "    tweet = filter_chars(tweet)\n",
    "    tweet = remove_mult_spaces(tweet)\n",
    "    tweet = remove_numbers(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    tweet = remove_short_words(tweet)\n",
    "    tweet = replace_elongated_words(tweet)\n",
    "    tweet = remove_repeated_punctuation(tweet)\n",
    "    tweet = remove_extra_whitespace(tweet)\n",
    "    tweet = remove_url_shorteners(tweet)\n",
    "    tweet = remove_spaces_tweets(tweet)\n",
    "    tweet = remove_short_tweets(tweet)\n",
    "    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c525e824-90b2-41f2-b6c6-4e1fac561829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = [clean_tweet(tweet) for tweet in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d8bfbae-2324-4ca1-9fd0-e439bd143c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean'].values\n",
    "clf = pickle.load(open('../model/clf.sav', 'rb'))\n",
    "X_cv =  clf.transform(X)\n",
    "tf_transformer = pickle.load(open('../model/tf_transformer.sav', 'rb'))\n",
    "X_tf = tf_transformer.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae47ea17-62b4-4a15-9c4a-22dda133c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and predict\n",
    "gb_classifier = pickle.load(open('../model/finalized_model.sav', 'rb'))\n",
    "df['sentiment'] = gb_classifier.predict(X_tf)\n",
    "df.to_csv('../prediction_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04c2c5-9239-462b-a923-d5866f8442d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
